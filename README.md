# Документация по системе обнаружения силуэтов.

## Содержание

Для отрендеренного вида любого файла Markdown в репозитории, включая файлы README, GitHub автоматически сгенерирует оглавление на основе заголовков разделов. Вы можете просмотреть оглавление для файла README, щелкнув значок меню в правом верхнем углу страницы.

## Обзор

### Введение
Эта документация предоставляет подробное описание системы обнаружения силуэтов человека и извлечения его атрибутов. Она предназначена для специалистов по данным, которые хотят понять, настроить, проверить и развернуть модель. Система обнаруживает силуэты на изображениях или видео с последующим определением атрибутов человека с использованием методов глубокого обучения.

### Цель
Цель этого документа - предоставить исчерпывающее руководство по:
- Пониманию архитектуры системы обнаружения объектов.
- Тонкой настройке предобученной модели.
- Проверке производительности модели.
- Оценке и развертыванию детектора в различных сценариях.

### Предварительные знания
Рекомендуется знание Python, фреймворка глубокого обучения PyTorch, а также основ обнаружения объектов для эффективного использования этой документации.

## Разделы

### 1. Установка и настройка

#### 1.1. Настройка окружения
Убедитесь, что установлена соответствующая версия Python. Клонируйте репозиторий и создайте виртуальное окружение:
```sh
git clone https://github.com/cbipok930/RTK-Silhouette-Detector
cd RTK-Silhouette-Detector
python -m venv env
source env/bin/activate  # На Windows используйте `env\Scripts\activate`
```

#### 1.2. Установка зависимостей
Установите все зависимости, необходимые для работы системы обнаружения объектов:
```sh
pip install -r requirements.txt
```

#### 1.3. Конфигурация системы
Система детектирования основана на модели *YOLOv8* и использует библиотеку onnxruntime для выполнения инференса. Определение атрибутов человека производится при помощи модели *ResNet* с использованием библиотеки PyTorch. 

Библиотека onnxruntime поддерживает несколько встроенных провайдеров, которые позволяют выбирать приоритетное устройство для выполнения инференса. Подробнее о провайдерах можно прочитать в официальной документации: провайдеры onnxruntime.

Выбор провайдеров настраивается в файле `detector.py`, где задается их порядок. Например, в текущей реализации используется провайдер CUDAExecutionProvider (для работы на GPU) с запасным вариантом в виде CPUExecutionProvider (для работы на CPU).

```python
class Detector(Base):
    """
    A base detector designed to be inherited by other detectors
    that use neural networks.

    Parameters
    ----------
    model_path : str
        Filename or serialized ONNX or ORT format model in a byte string.
    """

    CONF_TH = 0.3
    """The confidence threshold for the results of the onnx model."""
    IOU_TH = 0.7
    """The threshold of intersection over union for the results of the onnx model."""

    def __init__(self, model_path: str):
        super().__init__()
        self.session = ort.InferenceSession(
            model_path, providers=["CUDAExecutionProvider", "CPUExecutionProvider"]
        )
        """An onnx session to launch the model."""

        self.input_name: str = self.session.get_inputs()[0].name
        """The name of the input metadata."""
        self.output_name: list[str] = [self.session.get_outputs()[0].name]
        """The name of the output metadata."""
        self.lock = threading.Lock()
```
Настройки детектора *(в файле detector.py)*
* **CONF_TH** — Порог уверенности модели *YOLO*. Этот параметр определяет минимальный уровень уверенности, необходимый для того, чтобы обнаруженный объект считался валидным.
Значение задается в диапазоне от 0 до 1 *(по умолчанию 0.3)*.
* **IOU_TH** — Порог допустимого минимального перекрытия детектируемых объектов, при которым мы считаем, что перед нами один объект.

Настройка отправки детекций на определение атрибутов *(в файле silhouette_detector.py)*
* **score_th_capture** — определяет минимальную разницу между зафиксированным и постуающим значением уверенности дектции для данного целевого объекта. Этот параметр указывает на то, что при возрастании уверенности на заданный порог необходимо повторно определить атрибуты.
* **min_live_seconds** — определяет минимальный допустимый временной отрезок в секундах между первой детекцией данного целевого объетка и вызовом модели для определения атрибутов с этим целевым объектом. Этот параметр позволяет отсечь нестабильные детекции.
```python
class SilhouetteDetector(Detector):
    max_age_seconds_td = 60
    TDD: Dict[Any, TrackData] = {}
    TD_heartbeat = {}
    GenPD: Dict[Any, GenerationProcess] = {}
    GenP_heartbeat = {}

    max_age_seconds = 5
    score_th_capture = 0.2
    min_live_seconds = 0.3#2
```

### 2. Обзор модели
Детектор силуэтов основан на модели YOLOv8, одной из самых современных и мощных архитектур для детекции объектов. YOLOv8 предоставляет высокую точность и скорость обработки, что делает её подходящей для различных сценариев применения.

Для распознавания одного и того же силуэта на видео использется один из трекеров SFSORT или BoxMOT.

Для определения атрибутов человека используется ансамбль сверточных сетей ResNet, где каждая из нейронных сетей занимается классификацией соответствующего атрибута.

Исполнение детектора осуществляется двумя процессамим. Основной процесс включает в себя исполнение ядра системы обнаружения объектов с детектированием и трэкингом. Код, ответственный за классификацию атрибутов, расположен в файлах папки `generation_process`.
#### 2.1 Основной процесс
Описывается в `silhouette_detector.py`. Обработка каждого кадра осуществляется в методе **run** класса **SilhouetteDetector**. Обработтка изображений для получения детекций и их трекинга включает несколько этапов:
##### 2.1.1 Предобработка изображения
Предобработка изображения включает в себя:

* Преобразование цвета из BGR в RGB.
* Изменение размера (ресайз) изображения.
* Нормализацию данных.
* Преобразование пространств тензоров.

Все это реализовано в методе **pre_process**, который находится в файле `detector.py`.

##### 2.1.2 Инференс модели
После завершения этапа предобработки данные передаются в модель для выполнения инференса:
```
outputs = self.inference(pre_img)
```
Здесь метод **inference** отвечает за обработку данных в модели и возврат сырых результатов.

##### 2.1.3 Постобработка результатов
Постобработка выполняется в методе **post_process**, который находится в файле `detector.py.`

Этот этап включает разбор выходных данных, чтобы получить:

* Границы объектов (bounding boxes).
* Классы, к которым они относятся.
* Уверенность модели в предсказаниях.

##### 2.1.4 Трекинг детекций
Вызов предыдущих методов и трекера происходит с методе **run** класса **Base** из файла `base.py`. Метод возвращает список детекций, где каждый элемент — список из координат bounding boxes, идентификатора класса объекта, идентификатора трека, уверенности детекции.

##### 2.1.5 Фильтрация и взаимодействие со вспомогательным процессом

После выполнения родительского метода и получения списка детекций, выполняются следующие шаги:
- исключение из детекций иных классов, чем силуэт.
- фильтрация детекций для их отправки во вспомогательный процесс на определение атрибутов
- получение из вспомогательного процесса атрибутов для существующего объекта. По умолчанию каждый атрибут считается неопределенным.

На выходе возвращается список детекций, где в дополнение каждой детекции появляется словарь атрибутов. Ключи словаря перечислены ниже:
```python
ATT_KEYS = ['gender', 'age', 'headwear', 'top_color', 'bottom_color', 'top_type', 'bottom_type', 'backpack', 'handbag']
```
#### 2.2 Вспомогательный процесс
Описывается в функции **main** в файле `generation_process/__init__.py`
Исполнение начинается с инициализации модели ResNet и последующего выполнения цикла. Коммуникация с основным процессом осуществляется классом **GenerationProcess** .
##### 2.2.1 Получение изображений и предобработка
Из основого процесса на обработку приходит словарь с ключами *id* и *img*, означающими идентификатор трека и изображение силуэта, вырезанное по границам bounding box. Предобработка изображения происходит в методе **pred** класса **Classifier** в файле `generation_process/renset_ensamble.py`:
```python
...
    def pred(self, image, device='cuda'):
        _to_tensor = T.ToTensor()
        _central_crop = T.CenterCrop(224)
        _norm = T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)
        image = F.resize(image, int(min(image.size)*(224/(max(image.size)))), 3)
        image = _norm(_to_tensor(_central_crop(image)).float()).unsqueeze(0)
        image = image.to(device)
      ...
```
##### 2.2.2 Извлечение признаков из изображения
Осуществляется в методе **forward** класса **Classifier** в файле `generation_process/renset_ensamble.py`:
```python
    def forward(self, x):
        return [self.fc_0(torch.squeeze(self.resnet_0(x), (2, 3))),
                self.fc_1(torch.squeeze(self.resnet_1(x), (2, 3))),
                self.fc_2(torch.squeeze(self.resnet_2(x), (2, 3))),
                self.fc_3(torch.squeeze(self.resnet_3(x), (2, 3))),
                self.fc_4(torch.squeeze(self.resnet_4(x), (2, 3))),
                self.fc_5(torch.squeeze(self.resnet_5(x), (2, 3))),
                self.fc_6(torch.squeeze(self.resnet_6(x), (2, 3))),
                self.fc_7(torch.squeeze(self.resnet_7(x), (2, 3))),
                self.fc_8(torch.squeeze(self.resnet_8(x), (2, 3)))]
```
Метод **forward** вызывается в методе **pred**:
```python
   ...
        with torch.no_grad():
            preds = self(image)
   ...
```
##### 2.2.3 Классификация атрибутов по признакам
Происходит далее в методе **pred**:
```python
   ...
        if device=="cpu":
            preds = [np.argmax(o_i[0]) for o_i in [preds[self.output(jj)] for jj in range(9)]]
        else:
            preds = [torch.max(o_i[0], 0)[1].item() for o_i in preds]
        preds = [AttDataset.ATT_CLASSES_LABEL[AttDataset.ATT_LABEL[i]][preds[i]] for i in range(9)]
        return preds
```
### 3. Подготовка данных

#### 3.1. Сбор данных
- Руководство по сбору данных.

  Данные для обучения нашего детектора были собраны из общедоступных источников, а также предоставленных rtsp потоков.

  Полученные видео приходилось обрабтывать при помощи YOLO для получения обрезанных изображений (кропов) силуэтов. Чтобы исключить крайне похожие кропы, необходимо было использовать трекер в обработке видео. Требовалось также вручную либо автоматически через извлечение признаков исключать похожие изображения.

  В итоге должен получится датасет, состоящий из кропов силуэтов и аннотаций к ним.
- Разделение набора данных на обучающую, валидационную и тестовую выборки.

  Разбиение на выборки осуществляется достаточно просто. Следует выделить основной датасет, который мы разбиваем на обучающую и валидационную выборку, и тестовый датасет, где желательно присутствие кропов из тех видео, которые никаким образом не попадали в основной датасет. 

#### 3.2. Инструменты аннотации
Аннотирование атрибутов удобнее всего осуществлялось собственными инструментами, где можно было заполнять формы из заранее подготовленных вариантов.  
В общем случае, в том числе для аннотирования изображений для детекирования, можно использовать следующие стредства:
- Label Studio.
   * Универсальная платформа для аннотирования данных.
   * Поддерживает разметку изображений, текста, видео, и аудио.
   * Подходит для коллаборативной работы над проектами.
- CVAT (Computer Vision Annotation Tool)
   * Инструмент с открытым исходным кодом для разметки данных компьютерного зрения.
   * Поддерживает работу с изображениями и видео.
   * Обеспечивает удобные функции автоматизации аннотации, такие как отслеживание объектов.

#### 3.3. Аугментация данных
Для повышения устойчивости модели к различным вариациям входных данных применяются аугментации, которые включают следующие этапы обработки изображений, описанные в файле `generation_process/renset_ensamble.py`:

- Приведение данных к формату тензора (ToTensor)
Конвертирует изображения из формата NumPy (H x W x C) в тензоры PyTorch (C x H x W), а аннотации преобразуются в массив с фиксированным порядком атрибутов.

- Нормализация (Normalize)
Нормализует значения пикселей изображения, используя заданные параметры среднего (mean) и стандартного отклонения (std), соответствующие стандарту ImageNet.

- Изменение цветовых характеристик (ColorJitter)
Случайным образом изменяет яркость, контраст, насыщенность и оттенок изображения в заданных диапазонах.

- Регулировка резкости (Sharpness)
Случайным образом усиливает или ослабляет резкость изображения с заданным коэффициентом.

- Горизонтальное отражение (HorizontalFlip)
Применяет горизонтальное отражение изображения с вероятностью 50%.

- Аффинные преобразования (Affine)
Применяет случайные аффинные преобразования, включая вращение (до ±10°), сдвиг (до 5% от размеров изображения) и масштабирование (от 90% до 110% от оригинального размера).

Эти аугментации обеспечивают разнообразие обучающей выборки, что улучшает обобщающую способность модели.

### 4. Настройка и обучение модели
Пример всего пайплайна обучения присутствует в файле `generation_process/renset_ensamble.py`.
#### 4.1. Используемые фреймворки и библиотеки
Для обучения классификатора в данном проекте используются библиотеки PyTorch и torchvision. Эти инструменты обеспечивают эффективное создание нейронных сетей, обработку изображений, настройку и обучение модели.
#### Инструкции по установке и настройке
Для работы с кодом классификатора необходимо установить PyTorch и torchvision. Используйте следующую команду для установки через pip:
```
pip install torch torchvision
```
Также рекомендуется установить библиотеки для работы с изображениями и анализа данных:
```
pip install pillow matplotlib numpy
```

##### Загрузка весов и конфигураций
В файле `generation_process/renset_ensamble.py` описан класс **Classifier**. Инициализация модели выглядит так:
```
net = Classifier(backbone = "resnet50_backbone.pt")
```
В параметр backbone можно передавать путь до предобученной ResNet без выходного слоя для извлеченияя признаков. Получить подобную модель можно следующим образом:
```
import torchvision.models as models
import torch

resnet = models.resnet50(weights=None)
feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])
```

#### 4.2. Настройка гиперпараметров и конфигурационных файлов
Для настройки и обучения модели классификации важно определить значения гиперпараметров. Вот основные параметры и рекомендации:
##### Основные гиперпараметры
- Скорость обучения (learning_rate)

   - Рекомендуемое значение: 0.001 для стабильного обучения
   - Можно использовать оптимизатор Adam или SGD для более точной настройки.
   - Рекомедуется использование регуляризации L2 с коэфициентом 0.0001 
- Размер пакета (batch_size)
   - Количество изображений, обрабатываемых моделью одновременно.
   - Зависит от объёма памяти GPU:
      - Для GPU с 8 ГБ: 16.
      - Для GPU с 16 ГБ и выше: 32 или 64.
      - При ограниченной памяти можно использовать градиентный накопитель (gradient accumulation).
- Количество эпох (epochs)
   - Рекомендуемое значение: 100–200 для достижения сходимости
##### Аугментации данных
Для улучшения генерализации модели можно применять следующие преобразования:
- Геометрические трансформации:
   - Affine, HorizontalFlip, CenterCrop.
- Цветовые изменения:
   - ColorJitter, Normalize.
- Фильтры резкости:
   - Sharpness.
#### 4.3. Процесс обучения модели
Процесс обучения классификатора включает следующие шаги:
* Загрузка данных — Используйте класс **AttDataset** из `generation_process/renset_ensamble.py` для загрузки и обработки данных
* Определение функции потерь — используйте **AttributesLoss** из `generation_process/renset_ensamble.py`
* Обучение и валидация — используйте класс **Trainer** из `generation_process/renset_ensamble.py`
### 5. Проверка модели
#### 5.1 Ключевые метрики
1. Индивидуальная точность для каждого атрибута (attribute-specific accuracy)
   - Показывает, насколько хорошо модель справляется с классификацией конкретного атрибута
   - Полезна для диагностики: если точность низка для определённого атрибута, возможно, нужно больше данных или улучшение модели
2. Средняя точность (mean accuracy):
   - Главная метрика для общей оценки производительности модели
   - Удобна для сравнений между моделями

Если одна из категорий имеет низкую точность, это может указывать на дисбаланс в данных или сложности в отличии классов. Средняя точность является обобщением и может скрывать диспропорции между атрибутами. Поэтому важно анализировать как среднюю, так и индивидуальные точности

### 5.2. Процедура проверки

**Цель:**  
Обеспечить надёжную проверку модели в процессе обучения и сохранить промежуточные результаты для анализа и сравнения.

#### Шаги процедуры:

1. **Проверка на валидационных данных:**
   - После завершения каждой эпохи модель проверяется на валидационных данных:
     - **Подсчёт валидационных потерь:** Суммируются потери для всего набора валидационных данных.
     - **Вычисление метрик:** Оцениваются средний общий балл модели (score) и индивидуальные метрики для каждого атрибута.

2. **Сохранение результатов:**
   - После каждой пятой эпохи (или при досрочном завершении обучения) сохраняются:
     - Итоговые потери на тренировочных и валидационных данных.
     - Средний балл модели по всем атрибутам.
     - Метрики для каждого атрибута.
     - Описание архитектуры модели и параметры оптимизатора.
     - Веса модели.
---

### 5.3. Визуализация результатов

#### **Цель:**
Создание наглядных графиков, демонстрирующих динамику обучения и валидации, для анализа и диагностики модели. Процедура описана в файле `generation_process/renset_ensamble.py`

#### **Процесс визуализации:**

1. **График потерь (loss):**
   - Построение двух кривых:
     - **Потери на тренировке:** Красная кривая.
     - **Потери на валидации:** Синяя кривая.
   - Горизонтальная ось: Номер эпохи.
   - Вертикальная ось: Потери.
   - Добавление аннотации с итоговыми значениями потерь.

2. **График метрик (score):**
   - Построение кривой среднего балла по эпохам.
   - Горизонтальная ось: Номер эпохи.
   - Вертикальная ось: Значение среднего балла (score).
   - Добавление аннотации с итоговым значением среднего балла.

3. **Сохранение результатов:**
   - Все графики сохраняются в файл `plots.pdf` в указанной папке.

4. **Отображение:**
   - После построения графики отображаются на экране для немедленного анализа.

#### **Рекомендации:**
- Использовать графики для ранней диагностики проблем (например, отсутствие сходимости или переобучение).
- Сравнивать полученные графики для различных гиперпараметров или архитектур модели.

### 6. Отладка и оптимизация производительности

#### 6.1. Распространённые проблемы
##### Переобучение
Проблема: Модель демонстрирует отличные результаты на обучающем наборе, но плохо обобщает на валидационных или тестовых данных.

###### Как обнаружить переобучение:

Разрыв между значениями метрик и значениями потерь на обучающем и валидационном наборах.
Потери на обучении продолжают снижаться, но на валидации начинают расти.
###### Решения:

- Регуляризация:

 -- Используйте регуляризацию L2 (параметр weight decay) для предотвращения излишней подгонки.
- Аугментации данных:

 -- Увеличьте разнообразие данных с помощью агрессивных аугментаций, таких как вращение, изменение цвета, масштабирование и т.д.

- Раннее завершение обучения:
 -- Настройте раннюю остановку (early stopping), чтобы обучение завершалось при ухудшении метрик на валидации.

##### Недообучение
Проблема: Модель показывает низкие значения метрик как на обучающем, так и на валидационном наборе.

###### Решения:

- Увеличьте количество эпох

- Добавьте больше данных для обучения.
- Улучшите качество аннотаций.
- Оптимизация гиперпараметров:

 * Уменьшите learning rate, если модель не сходится.
 * Увеличьте размер пакета (batch size) для более стабильного обновления градиентов.


#### 6.2. Оптимизация производительности
Оптимизация производительности модели YOLO для детекции направлена на ускорение инференса, снижение использования памяти и адаптацию под целевые устройства. Рассмотрим подробно доступные методы и инструменты для различных аппаратных платформ.

##### 6.2.1. Ускорение инференса
###### TensorRT (NVIDIA)
TensorRT — это платформа от NVIDIA для оптимизации инференса на GPU. Она поддерживает такие оптимизации, как сжатие модели в формат FP16 или INT8 и фьюзинг операций.

Пример экспорта модели YOLO в TensorRT:
```
yolo export --weights path/to/best.pt --imgsz 640 --device 0 --format engine
```
Преимущества TensorRT:

* Значительное ускорение за счёт оптимизации вычислений.
* Поддержка работы на современных GPU NVIDIA (например, семейства Jetson).

###### OpenVINO (Intel)

OpenVINO — это платформа оптимизации инференса от Intel, которая поддерживает работу на процессорах Intel, GPU, и специализированных ускорителях, таких как Intel Movidius.

Пример экспорта модели в OpenVINO:
```
yolo export --weights path/to/best.pt --imgsz 640 --device 0 --format openvino
```

Особенности OpenVINO:

* Эффективное использование инструкций Intel AVX и VNNI.
* Подходит для серверов с Intel Xeon и устройств с Intel Core.

###### RKNN Toolkit (Rockchip)
Для устройств на базе процессоров Rockchip (например, RK3399, RK3588) можно использовать RKNN Toolkit. Этот инструмент позволяет оптимизировать модели для их запуска на NPU Rockchip.

Используйте RKNN Toolkit для преобразования ONNX в RKNN:
```
from rknn.api import RKNN

rknn = RKNN()
rknn.load_onnx('path/to/model.onnx')
rknn.build(do_quantization=True, dataset='path/to/dataset.txt')
rknn.export_rknn('model.rknn')
```

##### Квантование
Квантование снижает разрядность параметров модели (например, с FP32 до INT8), что уменьшает размер модели и ускоряет инференс. Поддерживается большинством фреймворков, включая TensorRT, OpenVINO, и RKNN.
```
yolo export --weights path/to/best.pt --imgsz 640 --device 0 --format engine --int8
```

### 7. Развертывание

#### 7.1. Экспорт Модели
Экспорт обученной модели является важным шагом для её развертывания на различных устройствах и платформах. В процессе экспорта модель преобразуется в формат, который оптимально подходит для целей инференса. Например, форматы ONNX или RKNN используются для работы на специализированных аппаратных ускорителях.

##### Экспорт в формат ONNX

ONNX (Open Neural Network Exchange) — это универсальный формат, который поддерживается большинством платформ, включая TensorRT, OpenVINO, и другие. Экспорт в ONNX осуществляется следующим образом:

```
from ultralytics import YOLO

# Загрузка обученной модели
model = YOLO('path/to/best.pt')

# Экспорт в формат ONNX
model.export(format='onnx')
```
После выполнения команды будет создан файл model.onnx, который можно использовать для оптимизации и инференса на различных устройствах.

##### Экспорт в формат RKNN
RKNN (Rockchip Neural Network) — это формат, предназначенный для работы на процессорах Rockchip с поддержкой NPU. Процесс экспорта состоит из двух шагов:
*  Конвертация модели в ONNX:

* Преобразование ONNX в RKNN с использованием RKNN Toolkit:
```
from rknn.api import RKNN
rknn = RKNN()
# Загрузка ONNX модели
rknn.load_onnx(model='path/to/model.onnx')
# Построение RKNN модели
rknn.build(do_quantization=True, dataset='path/to/dataset.txt')
# Экспорт в RKNN
rknn.export_rknn('model.rknn')
```

#### 7.2. Скрипт Инференса
Запуск run.sh

Выполните команду:

```
./run.sh
```
Дождитесь загрузки сервиса без ошибок.

Отправка тестового запроса

Используйте команду curl, чтобы проверить работу детектора, передав RTSP-поток или видеофайл. Пример:

```
curl -i -H "Content-Type: application/json" -X POST \
-d '{"cameraUrls": [{"video": "rtsp://192.168.1.100:554/live"}], "endTime": 1672531199}' \
http://localhost:8000/api/inference/1
```
Параметры:

* cameraUrls.video: URI видеопотока (RTSP).
* endTime: Метка времени в формате Unix Timestamp, соответствующая времени окончания анализа.

#### 7.3 Docker контейнеризация
# 7.3 Docker контейнеризация

Контейнеризация с помощью Docker позволяет изолировать среду для работы вашего приложения, что упрощает развертывание и обеспечивает воспроизводимость. Рассмотрим структуру вашего Dockerfile, аргументы для запуска и общий процесс контейнеризации.

---

## Описание вашего Dockerfile

1. **Базовый образ**
   ```dockerfile
   FROM nvidia/cuda:12.0.0-cudnn8-runtime-ubuntu22.04
   ```
   Вы используете образ с поддержкой **CUDA** и **cuDNN** для работы с NVIDIA GPU, что подходит для моделей, требующих ускорения на GPU.

2. **Установка системных зависимостей**
   ```dockerfile
   RUN apt-get update && apt-get install -y \\
       wget \\
       python3-dev \\
       python3-pip \\
       build-essential \\
       libssl-dev \\
       libffi-dev \\
       ffmpeg \\
       libsm6 \\
       libxext6 \\
       && rm -rf /var/lib/apt/lists/*
   ```
   Эти зависимости обеспечивают установку Python-библиотек, работу с видео (FFmpeg), и другие необходимые утилиты.

3. **Сборка FFmpeg**
   Вы выполняете сборку FFmpeg с поддержкой популярных кодеков (libx264, libx265, libopus и др.), что необходимо для обработки видеопотоков.

4. **Настройка проекта**
   - Вы задаёте рабочую директорию:  
     ```dockerfile
     WORKDIR /detector-api
     ```
   - Копируете зависимости:
     ```dockerfile
     COPY ./requirements.txt ./requirements.txt
     ```
   - Устанавливаете Python-зависимости:
     ```dockerfile
     RUN pip3 install -r ./requirements.txt
     ```
   - Копируете кодовую базу проекта:
     ```dockerfile
     COPY ./ ./
     ```

5. **Запуск проекта**
   В качестве точки входа используется `run.sh`:
   ```dockerfile
   ENTRYPOINT [ "bash", "run.sh" ]
   ```

6. **Expose порт**  
   Вы открываете порт `8000` для взаимодействия с API:
   ```dockerfile
   EXPOSE 8000
   ```

---

## Аргументы для запуска

1. **Сборка контейнера**
   Выполните команду для создания Docker-образа:
   ```bash
   docker build -t silhouette-detector-api:0.02 .
   ```

2. **Запуск контейнера**
   Запустите контейнер с подключением к GPU:
   ```bash
   docker run --name silhouette-detector-api-container --gpus device=1 -p 8104:8000 --rm -it -v ./logs/:/detector-api/logs/ -v ./videos/:/detector-api/videos/ silhouette-detector-api:0.02
   ```

---

## Оптимизация Dockerfile

Для повышения производительности и уменьшения размера образа можно использовать следующие подходы:

1. **Меньший базовый образ**
   Используйте `nvidia/cuda:12.0.0-cudnn8-runtime-ubuntu22.04` только если требуется поддержка CUDA. Если GPU не используется, выберите более лёгкий базовый образ, например, `python:3.10-slim`.

2. **Объединение команд для уменьшения количества слоёв**
   ```dockerfile
   RUN apt-get update && apt-get install -y \\
       wget python3-dev python3-pip build-essential libssl-dev libffi-dev \\
       ffmpeg libsm6 libxext6 && \\
       rm -rf /var/lib/apt/lists/*
   ```

3. **Установка Python-зависимостей из зафиксированного окружения**
   Для лучшей воспроизводимости используйте `requirements.txt` с точными версиями библиотек.




#### 7.4. Настройка pdoc для генерации API документации
- Использование pdoc для генерации документации по API, чтобы разработчики могли легко интегрировать информацию о функциях и их параметрах.
```sh
pip install pdoc
```
```sh
pdoc ./main.py -o docs/ -d numpy
```
pdoc принимает модуль и итеративно проходится по всем импортам для составления документации. Директория после аргумента -o указывает на выходные данные документации, а -d задает их вид (в данном случае NumPy style). Если требуется, можете установить темную тему, используя аргумент -t и путь до файлов CSS разметки из данного [репозитория](https://github.com/mitmproxy/pdoc/tree/main/examples/dark-mode).
- Это помогает поддерживать актуальную и легко доступную документацию по всем функциям и методам, что упрощает интеграцию и использование.

### 8. Ссылки

1. **Официальная документация PyTorch**  
   [https://docs.ultralytics.com/](https://docs.ultralytics.com/)  
   Здесь вы найдёте необходимые компоненты для обучения и рекомендации по их использованию.

4. **Репозиторий Rockchip RKNN Model Zoo**  
   [https://github.com/airockchip/rknn_model_zoo](https://github.com/airockchip/rknn_model_zoo)  
   Репозиторий содержит удобные инструменты и примеры для качественной конвертации моделей в формат RKNN для использования на процессорах Rockchip.

## Часто Задаваемые Вопросы (FAQ) и комментарии разработчика
---
